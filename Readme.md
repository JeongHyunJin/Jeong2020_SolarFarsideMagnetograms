Generation of solar farside magnetograms
=============

Solar farside magnetograms are generated by the Pix2PixHD model.      
We train and evaluate the model using pairs of SDO/AIA EUV passband images and SDO/HMI magnetograms.   
Then we generate realistic farside magnetograms from the corresponding EUV bassband images of STEREO/EUVIs by the model.

<p align="center">
<img src="https://user-images.githubusercontent.com/68056295/91798144-3137a500-ec5f-11ea-96d2-efd7cc208747.png" width="150%" height="150%"></center>
</p>

<br/>

_____________  

<br/>

Pix2PixHD model
===============
The Pix2PixHD is based on conditional Generative Adversarial Networks (cGANs) framework, which is one of the popular deep learning methods for image translation.

<p align="center">
<img src="https://user-images.githubusercontent.com/68056295/91625647-f2e28180-e9e3-11ea-8469-f9d4c932fe35.png" width="70%" height="70%"></center>
</p>

<br/>

Environments
------------
This code has been tested on Ubuntu 18.04 with a Nvidia GeForce GTX Titan XP GPU, CUDA Version 11.0, Python 3.6.9, and PyTorch 1.3.1.

<br/>

Network architectures
-------------
 The model consists of two major networks: 
 one is a generative network (generator) and the other is a discriminative network (discriminator).
    
 The generator tries to generate realistic output from input, and the discriminator tries to distinguish
 which one is a more real-like pair between a real pair and a fake pair.  

__Generator architectures__

    In our model, we use a global generator (G).
    The generator is consist of the Encoder - Residual Blocks - Decoder.
    The 'nd' indicate how many times you want to downsample input data, and the 'nr' indicate the number of residual blocks.

    - Encoder
         1. Conv2D(filter = 32, strides = 1), InstanceNorm2d, ReLU
         2. Conv2D(filter = 32*2^(i_nd+1), strides = 2), InstanceNorm2d, ReLU 

    - Residual Blocks (*nr)
         1. Conv2D(filter = 32*2^(nd+1), strides = 1), InstanceNorm2d, ReLU
         2. Conv2D(filter = 32*2^(nd+1), strides = 1), InstanceNorm2d

    - Decoder
         1. Conv2DTranspose(filter = 32*2^(nd+1)//2^(i_nd), strides = 2), InstanceNorm2d, ReLU
         2. Conv2DTranspose(filter = 32, strides = 1)
   
__Discriminator architectures__

    In our model, we use two 70*70 patch discriminator (D_1 and D_2).
    One discriminator gets input pairs of the original pixel size, and the other gets input pairs which are downsampled by half.

        1. Conv2D(filers = 64, strides = 2), LeakyReLu(slope = 0.2)
        2. Conv2D(filers = 128, strides = 2), InstanceNorm, LeakyReLu(slope = 0.2)
        3. Conv2D(filers = 256, strides = 2), InstanceNorm, LeakyReLu(slope = 0.2)
        4. Conv2D(filers = 512, strides = 2), InstanceNorm, LeakyReLu(slope = 0.2)
        5. Conv2D(filers = 1, strides = 1)
         


<br/>

Hyperparameters
-------------

__The Loss configuration of the Objective functions__    
* Total loss = ( cGAN loss ) + 10 * ( Feature Matching loss )   

__Optimizer__    
* Optimizer : Adam solver
* Learning rate : 0.0002
* momentum beta 1 parameter : 0.5
* momentum beta 2 parameter : 0.999   

__Initializer__  
* Initialize Weights in Convolutional Layers : normal distribution, mean : 0.0, standard deviation : 0.02   

<br/>

_____________  
